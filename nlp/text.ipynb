{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from pyltp import Segmentor, Postagger, Parser, NamedEntityRecognizer# 需装Python后再下载模型，此次运行的模型版本为3.4.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在加载LTP模型... ...\n",
      "加载模型完毕。\n"
     ]
    }
   ],
   "source": [
    "print(\"正在加载LTP模型... ...\")\n",
    "# Set your own model path\n",
    "MODELDIR=\"E:\\pyltp_model\\ltp_data_v3.4.0\"\n",
    "\n",
    "segmentor = Segmentor()\n",
    "# segmentor.load(os.path.join(MODELDIR, \"cws.model\"))\n",
    "cws_model_path=os.path.join(MODELDIR, \"cws.model\")\n",
    "segmentor.load_with_lexicon(cws_model_path, './dict.txt') #请把字典文件放进当前文件夹\n",
    "               \n",
    "postagger = Postagger()\n",
    "postagger.load(os.path.join(MODELDIR, \"pos.model\"))\n",
    "\n",
    "parser = Parser()\n",
    "parser.load(os.path.join(MODELDIR, \"parser.model\"))\n",
    "\n",
    "recognizer = NamedEntityRecognizer()\n",
    "recognizer.load(os.path.join(MODELDIR, \"ner.model\"))\n",
    "\n",
    "#labeller = SementicRoleLabeller()\n",
    "#labeller.load(os.path.join(MODELDIR, \"srl/\"))\n",
    "\n",
    "print(\"加载模型完毕。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence='小明是足球运动员，也是杰出的上海人，也是优秀的中国人'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "是 [0]\n"
     ]
    }
   ],
   "source": [
    "    # 切割句子\n",
    "    words = segmentor.segment(sentence)\n",
    "    # 词性标注\n",
    "    postags = postagger.postag(words)\n",
    "    # 命名实体识别\n",
    "    netags = recognizer.recognize(words, postags)\n",
    "    # 依存句法分析，其中已有模型可将句子中的主谓宾等结构抽取出来\n",
    "    arcs = parser.parse(words, postags)\n",
    "\n",
    "    child_dict_list = build_parse_child_dict(words, postags, arcs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1小明\n",
      "2是\n",
      "3足球\n",
      "4运动员\n",
      "5，\n",
      "6也\n",
      "7是\n",
      "8杰出\n",
      "9的\n",
      "10上海\n",
      "11人\n",
      "12，\n",
      "13也\n",
      "14是\n",
      "15优秀\n",
      "16的\n",
      "17中国\n",
      "18人\n"
     ]
    }
   ],
   "source": [
    "i=1\n",
    "for word in words:\n",
    "    print(str(i)+word)\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1nh\n",
      "2v\n",
      "3n\n",
      "4n\n",
      "5wp\n",
      "6d\n",
      "7v\n",
      "8a\n",
      "9u\n",
      "10ns\n",
      "11n\n",
      "12wp\n",
      "13d\n",
      "14v\n",
      "15a\n",
      "16u\n",
      "17ns\n",
      "18n\n"
     ]
    }
   ],
   "source": [
    "i=1\n",
    "for postag in postags:\n",
    "    print(str(i)+postag)\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1->2：SBV\n",
      "2->0：HED\n",
      "3->4：ATT\n",
      "4->2：VOB\n",
      "5->2：WP\n",
      "6->7：ADV\n",
      "7->2：COO\n",
      "8->11：ATT\n",
      "9->8：RAD\n",
      "10->11：ATT\n",
      "11->7：VOB\n",
      "12->2：WP\n",
      "13->14：ADV\n",
      "14->2：COO\n",
      "15->18：ATT\n",
      "16->15：RAD\n",
      "17->18：ATT\n",
      "18->14：VOB\n"
     ]
    }
   ],
   "source": [
    "i=1\n",
    "for arc in arcs:\n",
    "    print(str(i)+'->'+str(arc.head)+'：'+arc.relation)\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{},\n",
       " {'SBV': [0], 'VOB': [3], 'WP': [4, 11], 'COO': [6, 13]},\n",
       " {},\n",
       " {'ATT': [2]},\n",
       " {},\n",
       " {},\n",
       " {'ADV': [5], 'VOB': [10]},\n",
       " {'RAD': [8]},\n",
       " {},\n",
       " {},\n",
       " {'ATT': [7, 9]},\n",
       " {},\n",
       " {},\n",
       " {'ADV': [12], 'VOB': [17]},\n",
       " {'RAD': [15]},\n",
       " {},\n",
       " {},\n",
       " {'ATT': [14, 16]}]"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "child_dict_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_parse_child_dict(words, postags, arcs):\n",
    "    \"\"\"\n",
    "    为句子中的每个词语维护一个保存句法依存儿子节点的字典\n",
    "    Args:\n",
    "        words: 分词列表\n",
    "        postags: 词性列表\n",
    "        arcs: 句法依存列表\n",
    "    \"\"\"\n",
    "    child_dict_list = []\n",
    "    for index in range(len(words)):\n",
    "        child_dict = dict()\n",
    "        for arc_index in range(len(arcs)):\n",
    "            if arcs[arc_index].head == index + 1:\n",
    "#                 if child_dict.has_key(arcs[arc_index].relation):\n",
    "                if (arcs[arc_index].relation) in child_dict:\n",
    "                    child_dict[arcs[arc_index].relation].append(arc_index)\n",
    "                else:\n",
    "                    child_dict[arcs[arc_index].relation] = []\n",
    "                    child_dict[arcs[arc_index].relation].append(arc_index)\n",
    "        if ('SBV') in child_dict:\n",
    "            print(words[index],child_dict['SBV'])\n",
    "        child_dict_list.append(child_dict)\n",
    "    return child_dict_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "def complete_e(words, postags, child_dict_list, word_index):\n",
    "    \"\"\"\n",
    "    完善识别的部分实体\n",
    "    \"\"\"\n",
    "    child_dict = child_dict_list[word_index]\n",
    "    prefix = ''\n",
    "    if ('ATT') in child_dict:\n",
    "        for i in range(len(child_dict['ATT'])):\n",
    "            prefix = prefix+complete_e(words, postags, child_dict_list, child_dict['ATT'][i])\n",
    "    \n",
    "    postfix = ''\n",
    "    if postags[word_index] == 'v':\n",
    "        if ('VOB') in child_dict:\n",
    "            postfix =postfix+ complete_e(words, postags, child_dict_list, child_dict['VOB'][0])\n",
    "        if ('SBV') in child_dict:\n",
    "            prefix =prefix+ complete_e(words, postags, child_dict_list, child_dict['SBV'][0])\n",
    "\n",
    "    return prefix + words[word_index] + postfix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'SBV': [0], 'VOB': [3], 'WP': [4, 11], 'COO': [6, 13]}\n"
     ]
    }
   ],
   "source": [
    "index=1\n",
    "child_dict = child_dict_list[index]\n",
    "print(child_dict)\n",
    "e2=complete_e(words, postags, child_dict_list, child_dict['VOB'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "足球运动员\n"
     ]
    }
   ],
   "source": [
    "for i in filter(None, e2.split(',')):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'SBV': [0], 'VOB': [3], 'WP': [4, 11], 'COO': [6, 13]}\n",
      "小明\t是\t足球运动员\n",
      "\n",
      "{'ADV': [5], 'VOB': [10], 'SBV': [0]}\n",
      "小明\t是\t杰出上海人\n",
      "\n",
      "{'ADV': [12], 'VOB': [17], 'SBV': [0]}\n",
      "小明\t是\t优秀中国人\n",
      "\n"
     ]
    }
   ],
   "source": [
    "    for index in range(len(postags)):\n",
    "        # 抽取以谓词为中心的事实三元组\n",
    "        if postags[index] == 'v':# 找到谓语\n",
    "            child_dict = child_dict_list[index]\n",
    "            print(child_dict)\n",
    "            # 主谓宾\n",
    "            if   ('SBV') in child_dict and  ('VOB') in child_dict:\n",
    "                e1 = complete_e(words, postags, child_dict_list, child_dict['SBV'][0])\n",
    "                r = words[index]\n",
    "                e2 = complete_e(words, postags, child_dict_list, child_dict['VOB'][0])\n",
    "                print(\"%s\\t%s\\t%s\\n\" % (e1, r, e2)) #此处三元组的输出格式有待规范\n",
    "                # 此部分代码绝密，翻版必究-------------------------------------\n",
    "                if ('COO') in child_dict:\n",
    "                    for i in child_dict['COO']:\n",
    "                        child_dict_list[i]['SBV']=child_dict['SBV']\n",
    "                #----------------------------------------------------\n",
    "                # out_file.flush()\n",
    "            # 定语后置，动宾关系\n",
    "            \n",
    "            if arcs[index].relation == 'ATT':\n",
    "                if  ('VOB') in child_dict:\n",
    "                    e1 = complete_e(words, postags, child_dict_list, arcs[index].head - 1)\n",
    "                    r = words[index]\n",
    "                    e2 = complete_e(words, postags, child_dict_list, child_dict['VOB'][0])\n",
    "                    temp_string = r+e2\n",
    "                    if temp_string == e1[:len(temp_string)]:\n",
    "                        e1 = e1[len(temp_string):]\n",
    "                    if temp_string not in e1:\n",
    "                        print(\"%s\\t%s\\t%s\\n\" % (e1, r, e2)) #此处三元组的输出格式有待规范\n",
    "                        # out_file.flush()\n",
    "            # 含有介宾关系的主谓动补关系\n",
    "            if  ('SBV') in child_dict and  ('CMP') in child_dict:\n",
    "                #e1 = words[child_dict['SBV'][0]]\n",
    "                e1 = complete_e(words, postags, child_dict_list, child_dict['SBV'][0])\n",
    "                cmp_index = child_dict['CMP'][0]\n",
    "                r = words[index] + words[cmp_index]\n",
    "                if  ('POB') in child_dict_list[cmp_index]:\n",
    "                    e2 = complete_e(words, postags, child_dict_list, child_dict_list[cmp_index]['POB'][0])\n",
    "                    print(\"%s\\t%s\\t%s\\n\" % (e1, r, e2)) #此处三元组的输出格式有待规范\n",
    "                    # out_file.flush()\n",
    "\n",
    "        # 尝试抽取命名实体有关的三元组\n",
    "        if netags[index][0] == 'S' or netags[index][0] == 'B':\n",
    "            ni = index\n",
    "            if netags[ni][0] == 'B':\n",
    "                while netags[ni][0] != 'E':\n",
    "                    ni += 1\n",
    "                e1 = ''.join(words[index:ni+1])\n",
    "            else:\n",
    "                e1 = words[ni]\n",
    "            if arcs[ni].relation == 'ATT' and postags[arcs[ni].head-1] == 'n' and netags[arcs[ni].head-1] == 'O':\n",
    "                r = complete_e(words, postags, child_dict_list, arcs[ni].head-1)\n",
    "                if e1 in r:\n",
    "                    r = r[(r.index(e1)+len(e1)):]\n",
    "                if arcs[arcs[ni].head-1].relation == 'ATT' and netags[arcs[arcs[ni].head-1].head-1] != 'O':\n",
    "                    e2 = complete_e(words, postags, child_dict_list, arcs[arcs[ni].head-1].head-1)\n",
    "                    mi = arcs[arcs[ni].head-1].head-1\n",
    "                    li = mi\n",
    "                    if netags[mi][0] == 'B':\n",
    "                        while netags[mi][0] != 'E':\n",
    "                            mi += 1\n",
    "                        e = ''.join(words[li+1:mi+1])\n",
    "                        e2 += e\n",
    "                    if r in e2:\n",
    "                        e2 = e2[(e2.index(r)+len(r)):]\n",
    "                    if r+e2 in sentence:\n",
    "                        print(\"%s\\t%s\\t%s\\n\" % (e1, r, e2)) #此处三元组的输出格式有待规范\n",
    "                        # out_file.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "117"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arc.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'WP'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arc.relation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
