{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import Series,DataFrame\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import unicodedata\n",
    "from lxml.html import parse\n",
    "from urllib.request import urlopen\n",
    "\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "from util import get"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_introduction(soup):\n",
    "    \n",
    "    if soup.select('p')[0].get_text() == '抱歉，您所访问的页面不存在...':\n",
    "        return 0\n",
    "    \n",
    "    text = ''\n",
    "    for i in soup.select(\"div.lemma-summary\"):\n",
    "        for c in i:\n",
    "            if c != '\\n':\n",
    "                for t in [t for t in c.children if t.name != 'sup']:  #去掉【1】这种类型的标签\n",
    "                    text += t.string\n",
    "                \n",
    "    return text\n",
    "\n",
    "\n",
    "def get_basic_info(soup):\n",
    "    basic_info = {}\n",
    "    try:\n",
    "        for i in range(len(soup.select('div.basic-info')[0].select('dt'))) :\n",
    "            basic_info[soup.select('div.basic-info')[0].select('dt')[i].get_text()\n",
    "                      ] = soup.select('div.basic-info')[0].select('dd')[i].get_text()[1:-1]\n",
    "    except IndexError:\n",
    "        return np.nan\n",
    "            \n",
    "    return basic_info\n",
    "\n",
    "\n",
    "def get_name(soup):\n",
    "    return soup.select('div > dl > dd > h1')[0].string\n",
    "\n",
    "\n",
    "def parse( url):\n",
    "    if get(url) is None:\n",
    "        return\n",
    "    soup = BeautifulSoup(get(url).content.decode(), 'lxml')\n",
    "    dic = {}\n",
    "    dic['url'] = url\n",
    "    dic['name'] = get_name(soup)\n",
    "    dic['description'] = get_introduction(soup)\n",
    "    dic['metadata'] = get_basic_info( soup)\n",
    "    return dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_list = []\n",
    "with open(\"./companies_baidubaike/companies_baidubaike.txt\") as url_file:\n",
    "    for line in url_file.readlines():\n",
    "        url_list.append(line)\n",
    "\n",
    "os.mkdir(\"./companies_baidubaike/json_folder\")\n",
    "os.chdir(\"./companies_baidubaike/json_folder\")\n",
    "\n",
    "for u in url_list:\n",
    "    try:\n",
    "        with open(u.split('/')[-1]+'.json', 'w') as json_file:\n",
    "            json.dump(parse(u), json_file, indent=4)\n",
    "    except Exception:\n",
    "        pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
