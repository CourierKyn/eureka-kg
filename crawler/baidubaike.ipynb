{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from pandas import Series,DataFrame\n",
    "import numpy as np\n",
    "import unicodedata\n",
    "from lxml.html import parse\n",
    "from urllib.request import urlopen\n",
    "import re\n",
    "import json\n",
    "from util import get"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_introduction(soup):\n",
    "    \n",
    "    if soup.select('p')[0].get_text() == '抱歉，您所访问的页面不存在...':\n",
    "        return 0\n",
    "    \n",
    "    text = ''\n",
    "    for i in soup.select(\"div.lemma-summary\"):\n",
    "        for c in i:\n",
    "            if c != '\\n':\n",
    "                for t in [t for t in c.children if t.name != 'sup']:  #去掉【1】这种类型的标签\n",
    "                    text += t.string\n",
    "                \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_basic_info(soup):\n",
    "    basic_info = {}\n",
    "    try:\n",
    "        for i in range(len(soup.select('div.basic-info')[0].select('dt'))) :\n",
    "            basic_info[soup.select('div.basic-info')[0].select('dt')[i].get_text()\n",
    "                      ] = soup.select('div.basic-info')[0].select('dd')[i].get_text()[1:-1]\n",
    "    except IndexError:\n",
    "        return np.nan\n",
    "            \n",
    "    return basic_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_name(soup):\n",
    "    return soup.select('div > dl > dd > h1')[0].string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse( url):\n",
    "    if get(url) is None:\n",
    "        return\n",
    "    soup = BeautifulSoup(get(url).content.decode(), 'lxml')\n",
    "    dic = {}\n",
    "    dic['url'] = url\n",
    "    dic['name'] = get_name(soup)\n",
    "    dic['description'] = get_introduction(soup)\n",
    "    dic['metadata'] = get_basic_info( soup)\n",
    "    return dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "    url_list = ['https://baike.baidu.com/item/京东','https://baike.baidu.com/item/蚂蚁金服']\n",
    "    \n",
    "    for u in url_list:\n",
    "        \n",
    "         with open(u.split('/')[-1]+'.json', 'w') as json_file:\n",
    "            json.dump(parse(u), json_file, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
