{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from pandas import Series,DataFrame\n",
    "import numpy as np\n",
    "import unicodedata\n",
    "from lxml.html import parse\n",
    "from urllib.request import urlopen\n",
    "import re\n",
    "import json\n",
    "from util import get"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_introduction(soup):\n",
    "    \n",
    "    if soup.select('p')[0].get_text() == '抱歉，您所访问的页面不存在...':\n",
    "        return 0\n",
    "    \n",
    "    t = ''\n",
    "    for i in soup.select(\"div.lemma-summary\"):\n",
    "        t += i.get_text()\n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_basic_info(soup):\n",
    "    basic_info = {}\n",
    "    try:\n",
    "        for i in range(len(soup.select('div.basic-info')[0].select('dt'))) :\n",
    "            basic_info[soup.select('div.basic-info')[0].select('dt')[i].get_text()\n",
    "                      ] = soup.select('div.basic-info')[0].select('dd')[i].get_text()[1:-1]\n",
    "    except IndexError:\n",
    "        return np.nan\n",
    "            \n",
    "    return basic_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_name(soup):\n",
    "    return soup.select('div > dl > dd > h1')[0].string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse( url):\n",
    "    if get(url) is None:\n",
    "        return\n",
    "    soup = BeautifulSoup(get(url).content.decode(), 'lxml')\n",
    "    dic = {}\n",
    "    dic['url'] = url\n",
    "    dic['name'] = get_name(soup)\n",
    "    dic['description'] = get_introduction(soup)\n",
    "    dic['metadata'] = get_basic_info( soup)\n",
    "    return dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "    url_list = ['https://baike.baidu.com/item/蚂蚁金服','https://baike.baidu.com/item/京东']\n",
    "    \n",
    "    for u in url_list:\n",
    "        with open('百度百科.json','a', encoding='utf-8') as f:\n",
    "            # 设置不转换成ascii  json字符串首缩进\n",
    "            f.write( json.dumps( parse(u) ,ensure_ascii=False ,indent=2 ) )\n",
    "        \n",
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
